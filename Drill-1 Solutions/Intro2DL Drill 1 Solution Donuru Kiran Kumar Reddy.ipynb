{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows? : 7\n",
      "Columns : 7\n",
      "Populating the matrix with the given values of 0-6 \n",
      "\n",
      "The required matrix is \n",
      " [[0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]\n",
      " [0. 1. 2. 3. 4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question-1 : I want you to fill a matrix with values ranging from 0-6 by creating a 7*7 n-d array (matrix)\"\"\"\n",
    "\n",
    "\"\"\"I am dynamically reading the values of the dimentions. \n",
    "Simple the rows and colums the values will always start from 0-(columnsize -1)\"\"\"\n",
    "\n",
    "#Importing the required packages\n",
    "import numpy as np\n",
    "\n",
    "#This function is used to create a skeleton of the matrix with the required size\n",
    "def create_empty_ndarray(n,m):\n",
    "    nd_matrix = np.zeros((n,m))\n",
    "    return nd_matrix\n",
    "\n",
    "#This function is used to populate the skeliton matrix with the required values\n",
    "def populate_matrix(r,c):\n",
    "    matrix_n_n = create_empty_ndarray(r,c)\n",
    "    #print(\"Looking at the empty matrix\\n\", matrix_n_n)\n",
    "    print(\"Populating the matrix with the given values of 0-{} \\n\".format(c-1))\n",
    "    matrix_n_n += np.arange(c)\n",
    "    return matrix_n_n\n",
    "\n",
    "\n",
    "r = int(input(\"rows? : \"))\n",
    "c = int(input(\"Columns : \"))\n",
    "\n",
    "result_matrix = populate_matrix(r,c)\n",
    "print(\"The required matrix is \\n\",result_matrix)\n",
    "# print(\" The dimensions are :\",result_matrix.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the matrix with the given values of 0-2 \n",
      "\n",
      "[[0. 1. 2.]] \n",
      " (1, 3) \n",
      " (24, 8) \n",
      " 3 \n",
      "\n",
      "[120, 24, 40, 8]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question - 2 : Imagine you have 1-D ndarray A, make a  2-D ndarray for which the first row is [A[0],A[1],A[2]]\n",
    "and other following rows are shifted by unit of one for example the row in last will be shown as\n",
    "[A[-3],A[-2],A[-1])\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Assuimg the 1ND Array as an immage which has to be divided into patches  \"\"\"\n",
    "\n",
    "\"\"\"In deep learning we often need to preprocess inputs into patches. This can mean splitting an image into \n",
    "overlapping or non-overlapping 2D patches or splitting a long audio or text input into smaller \n",
    "equally sized chunks.\"\"\"\n",
    "\n",
    "\"\"\"Yes - we can make patch extraction fast and lightweight by leveraging how numpy (or torch, tf, jax, etc) \n",
    "stores arrays in memory. Numpy stores arrays \n",
    "in contiguous memory. This structure makes reading data from the image array very efficient.\n",
    "Instead of creating a new patches array, we only need to specify how to view the image as an array of patches.\n",
    "This is where the concept of strides comes in. You’ll notice that each numpy array has shape, \n",
    "strides, and itemsize properties. What do these properties mean?\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Shape = dimension of array. Itemsize = the number of bytes per cell (float32 = 32 bits = 4 bytes).\n",
    "Strides = the number of bytes required to traverse the contiguous memory block to read the next\n",
    "element along a given dimension.\n",
    "Strides = # steps-to-traverse * itemsize.\n",
    "If we know the output shape of the patch tensor, we can then specify the strides appropriately \n",
    "to get the desired patches. In numpy, the stride_tricks module \n",
    "provides this functionality. For example, here is how you implement \n",
    "non-overlapping patch extraction (e.g. for ViT).\"\"\"\n",
    "\n",
    "\"\"\"stride_tricks.as_strided   is a general purpose function for viewing any array. \n",
    "By specifying the shape and strides we fully define how to traverse this \n",
    "array for viewing. In our case we want to view it as patches - the main trick is to \n",
    "figure out the correct strides.\"\"\"\n",
    "\n",
    "\n",
    "from numpy.lib import stride_tricks\n",
    "\n",
    "img1 = np.array([[4,8,12],\n",
    "               [16,20,24],\n",
    "               [28,32,36]          \n",
    "])\n",
    "\n",
    "img = np.array([[4,8,12,16,20]])\n",
    "\n",
    "\n",
    "initial_matrix = populate_matrix(1,3)\n",
    "print(initial_matrix,\"\\n\",\n",
    "      initial_matrix.shape,\"\\n\",\n",
    "      initial_matrix.strides,\"\\n\",\n",
    "      initial_matrix.size,\"\\n\",\n",
    "     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#non-overlapping patches of size 8\n",
    "size = 3\n",
    "H, W = img.shape # taking height, width of the image\n",
    "shape = [H // size, W // size] + [size, size] #we are applying broadcasting here?\n",
    "\n",
    "\n",
    "#(row, col, patch_row, patch_col)\n",
    "strides = [size * s for s in img.strides] + list(img.strides)\n",
    "\n",
    "#extract patches\n",
    "patches = stride_tricks.as_strided(img, shape = shape, strides = strides)\n",
    "\n",
    "print(strides)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question - 2 : Imagine you have 1-D ndarray A, make a  2-D ndarray for which the first row is [A[0],A[1],A[2]]\n",
    "and other following rows are shifted by unit of one for example the row in last will be shown as\n",
    "[A[-3],A[-2],A[-1])\"\"\"\n",
    "\n",
    "\"\"\"Solving it with using strides concept\"\"\"\n",
    "\n",
    "img = np.array([[4,8,12,16,20,24]])\n",
    "\n",
    "siz = 3\n",
    "h, w = img.shape\n",
    "\n",
    "rows, colms = h//siz, w// siz\n",
    "colms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows? : 8\n",
      "Columns : 5\n",
      "Populating the matrix with the given values of 0-4 \n",
      "\n",
      "Generating 4-Dimensional ndarray \n",
      " [[0. 1. 2. 3. 4.]\n",
      " [1. 2. 3. 4. 0.]\n",
      " [2. 3. 4. 0. 1.]\n",
      " [3. 4. 0. 1. 2.]\n",
      " [4. 0. 1. 2. 3.]\n",
      " [0. 1. 2. 3. 4.]\n",
      " [1. 2. 3. 4. 0.]\n",
      " [2. 3. 4. 0. 1.]\n",
      " [3. 4. 0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question - 2 : Imagine you have 1-D ndarray A, make a  2-D ndarray for which the first row is [A[0],A[1],A[2]]\n",
    "and other following rows are shifted by unit of one for example the row in last will be shown as\n",
    "[A[-3],A[-2],A[-1])\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Coded this before asking question in the slack before my validations of my understanding\"\"\"\n",
    "\n",
    "\"\"\" Assuming I am required to generate a (N+1)Dimention ndarray from a given NDimention ndarray\"\"\"\n",
    "\n",
    "#used to rotate the array by one unit\n",
    "def rotate_the_array(arr):\n",
    "    res = np.append(arr[1:],(arr[0]))\n",
    "    return res\n",
    "\n",
    "\n",
    "#Using this function to generate a (n+1)darray from the given ndarray\n",
    "def generate_result_ndarray(n,m):\n",
    "    \n",
    "    #Using n+1 because adding the another array which creates N+1 darray\n",
    "    result_matrix = populate_matrix(n+1,m)\n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        #rotating one unit of previous array\n",
    "        result_matrix[i] = rotate_the_array(result_matrix[i-1])\n",
    "        \n",
    "    return result_matrix\n",
    "        \n",
    "    \n",
    "\n",
    "r = int(input(\"rows? : \"))\n",
    "c = int(input(\"Columns : \"))\n",
    "\n",
    "\n",
    "print(\"Generating {}-Dimensional ndarray \\n\".format(siz+1), generate_result_ndarray(r,c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"How can we save time using stochastic gradient compared to standard gradient descent? Do we have a better method?\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "'''In many big data setting (say several million data points), calculating cost or gradient takes very long time, because we need to sum over all data points.\n",
    "We do NOT need to have exact gradient to reduce the cost in a given iteration. Some approximation of gradient would work OK.\n",
    "Stochastic gradient decent (SGD) approximate the gradient using only one data point. So, evaluating gradient saves a lot of time compared to summing over all data.\n",
    "With \"reasonable\" number of iterations (this number could be couple of thousands, and much less than the number of data points, which may be millions), stochastic gradient decent may get a reasonable good solution.\n",
    "'''\n",
    "\n",
    "\"\"\"My notation follows Andrew NG's machine learning Coursera course. If you are not familiar with it, you can review the lecture series here.\n",
    "\n",
    "Let's assume regression on squared loss, the cost function is\n",
    "\n",
    "𝐽(𝜃)=12𝑚∑𝑖=1𝑚(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))2\n",
    "\n",
    "and the gradient is\n",
    "\n",
    "𝑑𝐽(𝜃)𝑑𝜃=1𝑚∑𝑖=1𝑚(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)\n",
    "\n",
    "for gradient decent (GD), we update the parameter by\n",
    "\n",
    "𝜃𝑛𝑒𝑤=𝜃𝑜𝑙𝑑−𝛼1𝑚∑𝑖=1𝑚(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)\n",
    "\n",
    "For stochastic gradient decent we get rid of the sum and 1/𝑚 constant, but get the gradient for current data point 𝑥(𝑖),𝑦(𝑖), where comes time saving.\n",
    "\n",
    "𝜃𝑛𝑒𝑤=𝜃𝑜𝑙𝑑−𝛼⋅(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)\n",
    "\n",
    "Here is why we are saving time:\n",
    "\n",
    "Suppose we have 1 billion data points.\n",
    "\n",
    "In GD, in order to update the parameters once, we need to have the (exact) gradient. This requires to sum up these 1 billion data points to perform 1 update.\n",
    "\n",
    "In SGD, we can think of it as trying to get an approximated gradient instead of exact gradient. The approximation is coming from one data point (or several data points called mini batch). Therefore, in SGD, we can update the parameters very quickly. In addition, if we \"loop\" over all data (called one epoch), we actually have 1 billion updates.\n",
    "\n",
    "The trick is that, in SGD you do not need to have 1 billion iterations/updates, but much less iterations/updates, say 1 million, and you will have \"good enough\" model to use.\n",
    "\n",
    "I am writing a code to demo the idea. We first solve the linear system by normal equation, then solve it with SGD. Then we compare the results in terms of parameter values and final objective function values. In order to visualize it later, we will have 2 parameters to tune.\n",
    "\n",
    "    set.seed(0);n_data=1e3;n_feature=2;\n",
    "    A=matrix(runif(n_data*n_feature),ncol=n_feature)\n",
    "    b=runif(n_data)\n",
    "    res1=solve(t(A) %*% A, t(A) %*% b)\n",
    "\n",
    "    sq_loss<-function(A,b,x){\n",
    "      e=A %*% x -b\n",
    "      v=crossprod(e)\n",
    "      return(v[1])\n",
    "    }\n",
    "\n",
    "    sq_loss_gr_approx<-function(A,b,x){\n",
    "      # note, in GD, we need to sum over all data\n",
    "      # here i is just one random index sample\n",
    "      i=sample(1:n_data, 1)\n",
    "      gr=2*(crossprod(A[i,],x)-b[i])*A[i,]\n",
    "      return(gr)\n",
    "    }\n",
    "\n",
    "    x=runif(n_feature)\n",
    "    alpha=0.01\n",
    "    N_iter=300\n",
    "    loss=rep(0,N_iter)\n",
    "\n",
    "    for (i in 1:N_iter){\n",
    "      x=x-alpha*sq_loss_gr_approx(A,b,x)\n",
    "      loss[i]=sq_loss(A,b,x)\n",
    "    }\n",
    "The results:\n",
    "\n",
    "as.vector(res1)\n",
    "[1] 0.4368427 0.3991028\n",
    "x\n",
    "[1] 0.3580121 0.4782659\n",
    "Note, although the parameters are not too close, the loss values are 124.1343 and 123.0355 which are very close.\n",
    "\n",
    "Here is the cost function values over iterations, we can see it can effectively decrease the loss, which illustrates the idea: we can use a subset of data to approximate the gradient and get \"good enough\" results.\n",
    "\n",
    "\n",
    "Now let's check the computational efforts between two approaches. In the experiment, we have 1000 data points, using SD, evaluate gradient once needs to sum over them data. BUT in SGD, sq_loss_gr_approx function only sum up 1 data point, and overall we see, the algorithm converges less than 300 iterations (note, not 1000 iterations.) This is the computational\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
